defaults:
  - _self_

# Task name for Anchor environment
task: "anchor"

# AnchorEnv parameters
# These will be passed directly to AnchorEnv.__init__()
X_unit: null  # Must be provided - normalized feature data [0, 1]
X_std: null   # Must be provided - standardized feature data
y: null       # Must be provided - class labels
feature_names: null  # Must be provided - list of feature names
classifier: null     # Must be provided - trained PyTorch classifier
device: "cpu"
target_class: null   # Optional: single target class
target_classes: null # Optional: list of target classes (defaults to all unique classes in y)

# Environment configuration dictionary
env_config:
  precision_target: 0.95
  coverage_target: 0.5
  use_perturbation: True
  perturbation_mode: "adaptive"  # "bootstrap", "uniform", or "adaptive"
  n_perturb: 4096
  step_fracs: [0.005, 0.01, 0.02]
  min_width: 0.05
  alpha: 0.7
  beta: 0.6
  gamma: 0.1
  precision_blend_lambda: 0.5
  drift_penalty_weight: 0.05
  min_coverage_floor: 0.005
  js_penalty_weight: 0.05
  initial_window: 0.1
  max_action_scale: 0.1
  min_absolute_step: 0.001
  inter_class_overlap_weight: 0.1  # Penalty weight for overlap between different agents' rules
  shared_reward_weight: 0.02  # Weight for shared cooperative reward (reduced from 0.2 to prevent high cumulative rewards)
  # Class union metrics weights: reward agents based on union of all agents' anchors for their class
  class_union_cov_weight: 0.01   # Weight for class-union coverage bonus (reduced from 0.1 to prevent high cumulative rewards)
  class_union_prec_weight: 0.01  # Weight for class-union precision bonus (reduced from 0.1 to prevent high cumulative rewards)
  # Global coverage reward: reward when all agents together cover the dataset well
  global_coverage_weight: 0.01    # Weight for global coverage bonus (reduced from 0.1 to prevent high cumulative rewards)
  global_coverage_threshold: 0.1 # Minimum global coverage required to receive bonus (0.0 = reward any positive coverage)
  # Coverage bonus weights (reduced from original values to prevent high cumulative rewards)
  # These bonuses reward good coverage behavior (same as single-agent for fair comparison)
  coverage_bonus_weight_met: 0.01          # When targets are met: weight * (coverage / target)
  coverage_bonus_weight_high_prec: 0.03    # When precision >= threshold and making progress: base weight
  coverage_bonus_weight_high_prec_progress: 0.07  # Progress multiplier when precision >= threshold
  coverage_bonus_weight_high_prec_distance: 0.02  # Distance to target multiplier when precision >= threshold
  coverage_bonus_weight_reasonable_prec: 0.01     # When precision >= 0.8 * threshold and making progress: base weight
  coverage_bonus_weight_reasonable_prec_progress: 0.02  # Progress multiplier when precision >= 0.8 * threshold
  # Target class bonus weight (reduced from 0.2 to prevent high cumulative rewards, same as single-agent)
  target_class_bonus_weight: 0.02  # Weight for target class fraction bonus
  agents_per_class: 3        # Number of agents per class (set >1 to learn multiple anchors per class)
  # Use class centroids to initialize the initial window (default: true)
  # If true: initial window is set around class centroid (mean of class instances)
  # If false: initial window is full space (0 to 1 for all features)
  # For instance-based: x_star_unit overrides this (instance centroid is used)
  use_class_centroids: true
  # Optional: Precomputed cluster centroids per class (dict of {class: [centroid1, centroid2, ...]})
  # If provided, a random centroid is sampled for each episode reset
  # If None, mean centroid is computed from class data
  cluster_centroids_per_class: null
  # Optional: Fixed instances per class to use as centroids (dict of {class: [instance1, instance2, ...]})
  # If provided, a random instance is sampled as centroid for each episode reset
  # If None, mean centroid is computed from class data
  fixed_instances_per_class: null

# Logging verbosity control
# Set to "verbose" for detailed debug logs, "normal" for standard logs, "quiet" for minimal logs
# Controls logging in training, inference, and environment during rollouts
logging_verbosity: "normal"  # Options: "quiet", "normal", "verbose"

# Maximum number of steps per episode
# Increased to 500 for better convergence (allows agents more time to refine anchors)
max_cycles: 500

# Termination reason counters: disable overused reasons
# Strategy: Higher limits for better outcomes, lower limits for easier/less ideal outcomes
max_termination_count_both_targets: -1
max_termination_count_high_precision: 200
max_termination_count_both_close: 50
max_termination_count_excellent_precision: 30

