defaults:
  - _self_

# Task name for Anchor environment
task: "anchor"

# AnchorEnv parameters
# These will be passed directly to AnchorEnv.__init__()
X_unit: null  # Must be provided - normalized feature data [0, 1]
X_std: null   # Must be provided - standardized feature data
y: null       # Must be provided - class labels
feature_names: null  # Must be provided - list of feature names
classifier: null     # Must be provided - trained PyTorch classifier
device: "cpu"
target_class: null   # Optional: single target class
target_classes: null # Optional: list of target classes (defaults to all unique classes in y)

# Environment configuration dictionary
env_config:
  precision_target: 0.95  # Reduced from 0.99 to allow boxes to expand more for better coverage
  coverage_target: 0.5
  use_perturbation: True
  perturbation_mode: "adaptive"  # "bootstrap", "uniform", or "adaptive"
  n_perturb: 4096
  step_fracs: [0.005, 0.01, 0.02]
  min_width: 0.05
  alpha: 0.7
  beta: 1.0  # Increased from 0.6 to 1.0 to give coverage equal or higher weight than precision
  gamma: 0.02
  precision_blend_lambda: 0.5
  drift_penalty_weight: 0.01
  min_coverage_floor: 0.1  # Increased from 0.005 to 0.01 to prevent boxes from becoming too small
  js_penalty_weight: 0.01
  # Episode length (kept here so it is passed directly into AnchorEnv.__init__)
  max_cycles: 200  # Increased from 100 to allow boxes more time to expand for better coverage

  # Stabilization-based early termination (per-agent)
  enable_stability_termination: true
  stability_window: 20  # Increased from 10 to require more stable steps before early termination
  stability_min_steps: 50  # Increased from 20 to prevent premature termination during expansion phase
  stability_precision_tol: 1e-3
  stability_coverage_tol: 1e-3
  stability_drift_tol: 1e-3

  initial_window: 0.1
  max_action_scale: 0.15  # Increased from 0.1 to 0.15 to allow faster box expansion for better coverage
  min_absolute_step: 0.01
  inter_class_overlap_weight: 0.1  # Penalty weight for overlap between different agents' rules
  shared_reward_weight: 0.5  # Weight for shared cooperative reward - MUST be significant to encourage cooperation
  # If shared_reward_weight is too low, multi-agent training becomes equivalent to independent single-agent training
  # The shared reward incentivizes agents to work together and achieve collective goals
  # Class union metrics weights: reward agents based on union of all agents' anchors for their class
  # Significantly increased to encourage better coverage through coordination.
  class_union_cov_weight: 0.3   # Weight for class-union coverage bonus (increased from 0.05 to strongly incentivize coverage)
  class_union_prec_weight: 0.3  # Weight for class-union precision bonus (increased from 0.02)
  # Global coverage reward: reward when all agents together cover the dataset well
  # Significantly increased to encourage overall coverage improvement.
  global_coverage_weight: 0.10    # Weight for global coverage bonus (increased from 0.03)
  global_coverage_threshold: 0.8 # Minimum global coverage required to receive bonus (0.0 = reward any positive coverage)
  # Coverage bonus weights - SIGNIFICANTLY INCREASED to incentivize coverage expansion
  # These bonuses reward good coverage behavior and encourage agents to expand boxes
  coverage_bonus_weight_met: 0.05          # When targets are met: weight * (coverage / target) - Increased from 0.01
  coverage_bonus_weight_high_prec: 0.10    # When precision >= threshold and making progress: base weight - Increased from 0.03
  coverage_bonus_weight_high_prec_progress: 0.20  # Progress multiplier when precision >= threshold - Increased from 0.07
  coverage_bonus_weight_high_prec_distance: 0.10  # Distance to target multiplier when precision >= threshold - Increased from 0.02
  coverage_bonus_weight_reasonable_prec: 0.05     # When precision >= 0.8 * threshold and making progress: base weight - Increased from 0.01
  coverage_bonus_weight_reasonable_prec_progress: 0.30  # Progress multiplier when precision >= 0.8 * threshold - Increased from 0.02
  # Target class bonus weight (reduced from 0.2 to prevent high cumulative rewards, same as single-agent)
  target_class_bonus_weight: 0.02  # Weight for target class fraction bonus
  # NashConv threshold for model selection (ε-Nash equilibrium)
  # Models with NashConv <= this threshold are considered close to Nash equilibrium
  # Lower values = stricter equilibrium requirement (default: 0.01 = 1% exploitability)
  nashconv_threshold: 0.01  # ε-Nash threshold for best model selection
  agents_per_class: 4        # Number of agents per class (set >1 to learn multiple anchors per class)
  # IMPORTANT: This should match the training configuration. If you trained with agents_per_class: 3,
  # use 3 here to use all individual policies. If you trained with agents_per_class: 1, use 1 here.
  # The inference code now properly uses individual policies for each agent when agents_per_class > 1.
  # Use class centroids to initialize the initial window (default: true)
  # If true: initial window is set around class centroid (mean of class instances)
  # If false: initial window is full space (0 to 1 for all features)
  # For instance-based: x_star_unit overrides this (instance centroid is used)
  use_class_centroids: true
  # Optional: Precomputed cluster centroids per class (dict of {class: [centroid1, centroid2, ...]})
  # If provided, a random centroid is sampled for each episode reset
  # If None, mean centroid is computed from class data
  cluster_centroids_per_class: null
  # Optional: Fixed instances per class to use as centroids (dict of {class: [instance1, instance2, ...]})
  # If provided, a random instance is sampled as centroid for each episode reset
  # If None, mean centroid is computed from class data
  fixed_instances_per_class: null
  # Training initialization: ratio of instance-based vs centroid-based episodes
  # 0.0 = all centroid-based, 1.0 = all instance-based, 0.1 = 10% instance-based, 90% centroid-based
  training_instance_ratio: 0.1  # 10% instance-based (prediction matching), 90% class-based (class correctness)
  # Adaptive class-specific ratios: automatically increase ratio for minority classes in imbalanced datasets
  use_adaptive_instance_ratios: true  # If true, uses higher ratios for minority classes (recommended for imbalanced datasets)

# Logging verbosity control
# Set to "verbose" for detailed debug logs, "normal" for standard logs, "quiet" for minimal logs
# Controls logging in training, inference, and environment during rollouts
logging_verbosity: "normal"  # Options: "quiet", "normal", "verbose"

# Maximum number of steps per episode (source of truth is env_config.max_cycles; kept here for backward compatibility)
# If your task wrapper reads only the top-level max_cycles, keep both in sync.
max_cycles: 200  # Increased from 100 to allow boxes more time to expand for better coverage

# Termination reason counters: disable overused reasons
# Strategy: Higher limits for better outcomes, lower limits for easier/less ideal outcomes
max_termination_count_both_targets: -1
max_termination_count_high_precision: 200
max_termination_count_both_close: 50
max_termination_count_excellent_precision: 30
