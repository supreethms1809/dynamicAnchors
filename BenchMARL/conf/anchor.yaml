defaults:
  - _self_

# Task name for Anchor environment
task: "anchor"

# AnchorEnv parameters
# These will be passed directly to AnchorEnv.__init__()
X_unit: null  # Must be provided - normalized feature data [0, 1]
X_std: null   # Must be provided - standardized feature data
y: null       # Must be provided - class labels
feature_names: null  # Must be provided - list of feature names
classifier: null     # Must be provided - trained PyTorch classifier
device: "cpu"
target_class: null   # Optional: single target class
target_classes: null # Optional: list of target classes (defaults to all unique classes in y)

# Environment configuration dictionary
env_config:
  precision_target: 0.95
  coverage_target: 0.2
  use_perturbation: True
  perturbation_mode: "adaptive"  # "bootstrap", "uniform", or "adaptive"
  n_perturb: 4096
  step_fracs: [0.005, 0.01, 0.02]
  min_width: 0.05
  alpha: 0.7
  beta: 0.6  # Reduced from 1.0 to prevent over-expansion and reward explosion
  gamma: 0.1  # Increased from 0.02 to prevent box collapse
  precision_blend_lambda: 0.5
  drift_penalty_weight: 0.05  # Increased from 0.01 for more stable boxes
  min_coverage_floor: 0.005
  js_penalty_weight: 0.05  # Increased from 0.01 to prevent distribution drift
  # Episode length (kept here so it is passed directly into AnchorEnv.__init__)
  max_cycles: 500  # Increased from 100 to allow boxes more time to expand for better coverage

  # Stabilization-based early termination (per-agent)
  enable_stability_termination: true
  stability_window: 20  # Increased from 10 to require more stable steps before early termination
  stability_min_steps: 50  # Increased from 20 to prevent premature termination during expansion phase
  stability_precision_tol: 1e-3
  stability_coverage_tol: 1e-3
  stability_drift_tol: 1e-3

  initial_window: 0.1
  max_action_scale: 0.1  # Reduced from 0.15 for more stable, gradual expansion
  min_absolute_step: 0.001  # Reduced from 0.01 for finer control
  inter_class_overlap_weight: 0.1  # Penalty weight for overlap between different agents' rules
  shared_reward_weight: 0.5  # Weight for shared cooperative reward - MUST be significant to encourage cooperation
  # If shared_reward_weight is too low, multi-agent training becomes equivalent to independent single-agent training
  # The shared reward incentivizes agents to work together and achieve collective goals
  # Class union metrics weights: reward agents based on union of all agents' anchors for their class
  # Reduced to prevent reward explosion and unstable training
  class_union_cov_weight: 0.01   # Weight for class-union coverage bonus (reduced from 0.3 to prevent high cumulative rewards)
  class_union_prec_weight: 0.01  # Weight for class-union precision bonus (reduced from 0.3 to prevent high cumulative rewards)
  # Global coverage reward: reward when all agents together cover the dataset well
  # Reduced to prevent reward explosion
  global_coverage_weight: 0.01    # Weight for global coverage bonus (reduced from 0.10 to prevent high cumulative rewards)
  global_coverage_threshold: 0.1 # Minimum global coverage required to receive bonus (reduced from 0.8)
  # Coverage bonus weights (reduced from original values to prevent high cumulative rewards)
  # These bonuses reward good coverage behavior (same as single-agent for fair comparison)
  coverage_bonus_weight_met: 0.1          # When targets are met: weight * (coverage / target)
  coverage_bonus_weight_high_prec: 0.1    # When precision >= threshold and making progress: base weight
  coverage_bonus_weight_high_prec_progress: 0.07  # Progress multiplier when precision >= threshold (reduced from 0.20)
  coverage_bonus_weight_high_prec_distance: 0.02  # Distance to target multiplier when precision >= threshold
  coverage_bonus_weight_reasonable_prec: 0.01     # When precision >= 0.8 * threshold and making progress: base weight
  coverage_bonus_weight_reasonable_prec_progress: 0.02  # Progress multiplier when precision >= 0.8 * threshold (reduced from 0.30)
  # Target class bonus weight (reduced from 0.2 to prevent high cumulative rewards, same as single-agent)
  target_class_bonus_weight: 0.02  # Weight for target class fraction bonus
  # NashConv threshold for model selection (ε-Nash equilibrium)
  # Models with NashConv <= this threshold are considered close to Nash equilibrium
  # Lower values = stricter equilibrium requirement (default: 0.01 = 1% exploitability)
  nashconv_threshold: 0.01  # ε-Nash threshold for best model selection
  agents_per_class: 3        # Number of agents per class (increased from 2 for better diversity)
  # IMPORTANT: This should match the training configuration. If you trained with agents_per_class: 3,
  # use 3 here to use all individual policies. If you trained with agents_per_class: 1, use 1 here.
  # The inference code now properly uses individual policies for each agent when agents_per_class > 1.
  # Use class centroids to initialize the initial window (default: true)
  # If true: initial window is set around class centroid (mean of class instances)
  # If false: initial window is full space (0 to 1 for all features)
  # For instance-based: x_star_unit overrides this (instance centroid is used)
  use_class_centroids: true
  # Optional: Precomputed cluster centroids per class (dict of {class: [centroid1, centroid2, ...]})
  # If provided, a random centroid is sampled for each episode reset
  # If None, mean centroid is computed from class data
  cluster_centroids_per_class: null
  # Optional: Fixed instances per class to use as centroids (dict of {class: [instance1, instance2, ...]})
  # If provided, a random instance is sampled as centroid for each episode reset
  # If None, mean centroid is computed from class data
  fixed_instances_per_class: null
  # Training initialization: ratio of instance-based vs centroid-based episodes
  # 0.0 = all centroid-based, 1.0 = all instance-based, 0.1 = 10% instance-based, 90% centroid-based
  training_instance_ratio: 1.0  # 10% instance-based (prediction matching), 90% class-based (class correctness)
  # Adaptive class-specific ratios: automatically increase ratio for minority classes in imbalanced datasets
  use_adaptive_instance_ratios: true  # If true, uses higher ratios for minority classes (recommended for imbalanced datasets)
  
  # Evaluation settings
  # Set to false to use training data for evaluation (matches baseline behavior)
  # When coverage_on_all_data flag is used in inference, coverage is computed on all data (train+test combined)
  eval_on_test_data: false
  
  # Termination reason counters: disable overused reasons
  # Strategy: Higher limits for better outcomes, lower limits for easier/less ideal outcomes
  max_termination_count_both_targets: -1
  max_termination_count_high_precision: 200
  max_termination_count_both_close: 100
  max_termination_count_excellent_precision: 100

# Logging verbosity control
# Set to "verbose" for detailed debug logs, "normal" for standard logs, "quiet" for minimal logs
# Controls logging in training, inference, and environment during rollouts
logging_verbosity: "normal"  # Options: "quiet", "normal", "verbose"

# Maximum number of steps per episode (source of truth is env_config.max_cycles; kept here for backward compatibility)
# If your task wrapper reads only the top-level max_cycles, keep both in sync.
max_cycles: 500  # Increased from 100 to allow boxes more time to expand for better coverage


