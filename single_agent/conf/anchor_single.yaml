# Single-Agent Anchor Environment Configuration
# This config file defines all reward weights and environment settings for single-agent training

# Environment configuration dictionary
env_config:
  # Target metrics
  precision_target: 0.95
  coverage_target: 0.2
  
  # Data perturbation settings
  use_perturbation: True
  perturbation_mode: "adaptive"  # "bootstrap", "uniform", or "adaptive"
  n_perturb: 4096
  
  # Action and step settings
  step_fracs: [0.005, 0.01, 0.02]
  min_width: 0.05
  max_action_scale: 0.1
  min_absolute_step: 0.001
  initial_window: 0.1
  
  # Reward weight parameters
  alpha: 0.7              # Weight for precision gain component
  beta: 0.6               # Weight for coverage gain component (reduced from 1.0 to prevent over-expansion)
  gamma: 0.1              # Weight for overlap penalty (increased from 0.02 to prevent box collapse)
  precision_blend_lambda: 0.5
  drift_penalty_weight: 0.05  # Increased from 0.01 for more stable boxes
  js_penalty_weight: 0.05  # Increased from 0.01 to prevent distribution drift
  min_coverage_floor: 0.005  # Lower default (will be set dynamically in inference to 1/n_samples)
  
  # Coverage bonus weights (reduced from original values to prevent high cumulative rewards)
  # These bonuses reward good coverage behavior
  coverage_bonus_weight_met: 0.01          # When targets are met: weight * (coverage / target) (reduced from 0.05)
  coverage_bonus_weight_high_prec: 0.03    # When precision >= threshold and making progress: base weight (reduced from 0.10)
  coverage_bonus_weight_high_prec_progress: 0.07  # Progress multiplier when precision >= threshold (reduced from 0.20)
  coverage_bonus_weight_high_prec_distance: 0.02  # Distance to target multiplier when precision >= threshold
  coverage_bonus_weight_reasonable_prec: 0.01     # When precision >= 0.8 * threshold and making progress: base weight
  coverage_bonus_weight_reasonable_prec_progress: 0.02  # Progress multiplier when precision >= 0.8 * threshold (reduced from 0.30)
  
  # Target class bonus weight (reduced from 0.2 to prevent high cumulative rewards)
  target_class_bonus_weight: 0.02  # Weight for target class fraction bonus
  
  # Multi-agent compatibility fields (not used in single-agent, but kept for API consistency)
  inter_class_overlap_weight: 0.0   # Not applicable: single agent per environment
  shared_reward_weight: 0.0         # Not applicable: no shared rewards in single-agent
  class_union_cov_weight: 0.0       # Not applicable: single agent per class
  class_union_prec_weight: 0.0      # Not applicable: single agent per class
  global_coverage_weight: 0.0       # Not applicable: single agent per class
  
  # Initialization settings
  use_class_centroids: true
  cluster_centroids_per_class: null
  fixed_instances_per_class: null
  use_random_sampling: false
  # Training initialization: ratio of instance-based vs centroid-based episodes
  # 0.0 = all centroid-based, 1.0 = all instance-based, 0.1 = 10% instance-based, 90% centroid-based
  training_instance_ratio: 1.0  # 10% instance-based (prediction matching), 90% class-based (class correctness)
  # Adaptive class-specific ratios: automatically increase ratio for minority classes in imbalanced datasets
  use_adaptive_instance_ratios: true  # If true, uses higher ratios for minority classes (recommended for imbalanced datasets)
  
  # Evaluation settings
  # Set to false to use training data for evaluation (matches baseline behavior)
  # When coverage_on_all_data flag is used in inference, coverage is computed on all data (train+test combined)
  eval_on_test_data: false
  
  # Episode length (kept here so it is passed directly into SingleAgentAnchorEnv.__init__)
  max_cycles: 500  # Increased from 100 to allow boxes more time to expand for better coverage

# Logging verbosity control
# Set to "verbose" for detailed debug logs, "normal" for standard logs, "quiet" for minimal logs
# Controls logging in training, inference, and environment during rollouts
logging_verbosity: "normal"  # Options: "quiet", "normal", "verbose"

# Maximum number of steps per episode
max_cycles: 500  # Increased from 100 to allow boxes more time to expand for better coverage

# Termination reason counters: disable overused reasons
# Strategy: Higher limits for better outcomes, lower limits for easier/less ideal outcomes
max_termination_count_both_targets: -1
max_termination_count_high_precision: 200
max_termination_count_both_close: 50
max_termination_count_excellent_precision: 30
