# Single-Agent Anchor Environment Configuration
# This config file defines all reward weights and environment settings for single-agent training

# Environment configuration dictionary
env_config:
  # Target metrics
  precision_target: 0.95
  coverage_target: 0.5
  
  # Data perturbation settings
  use_perturbation: True
  perturbation_mode: "adaptive"  # "bootstrap", "uniform", or "adaptive"
  n_perturb: 4096
  
  # Action and step settings
  step_fracs: [0.005, 0.01, 0.02]
  min_width: 0.05
  max_action_scale: 0.1
  min_absolute_step: 0.001
  initial_window: 0.1
  
  # Reward weight parameters
  alpha: 0.7              # Weight for precision gain component
  beta: 0.6               # Weight for coverage gain component
  gamma: 0.1              # Weight for overlap penalty
  precision_blend_lambda: 0.5
  drift_penalty_weight: 0.05
  js_penalty_weight: 0.05
  min_coverage_floor: 0.005
  
  # Coverage bonus weights (reduced from original values to prevent high cumulative rewards)
  # These bonuses reward good coverage behavior
  coverage_bonus_weight_met: 0.01          # When targets are met: weight * (coverage / target)
  coverage_bonus_weight_high_prec: 0.03    # When precision >= threshold and making progress: base weight
  coverage_bonus_weight_high_prec_progress: 0.07  # Progress multiplier when precision >= threshold
  coverage_bonus_weight_high_prec_distance: 0.02  # Distance to target multiplier when precision >= threshold
  coverage_bonus_weight_reasonable_prec: 0.01     # When precision >= 0.8 * threshold and making progress: base weight
  coverage_bonus_weight_reasonable_prec_progress: 0.02  # Progress multiplier when precision >= 0.8 * threshold
  
  # Target class bonus weight (reduced from 0.2 to prevent high cumulative rewards)
  target_class_bonus_weight: 0.02  # Weight for target class fraction bonus
  
  # Multi-agent compatibility fields (not used in single-agent, but kept for API consistency)
  inter_class_overlap_weight: 0.0   # Not applicable: single agent per environment
  shared_reward_weight: 0.0         # Not applicable: no shared rewards in single-agent
  class_union_cov_weight: 0.0       # Not applicable: single agent per class
  class_union_prec_weight: 0.0      # Not applicable: single agent per class
  global_coverage_weight: 0.0       # Not applicable: single agent per class
  
  # Initialization settings
  use_class_centroids: true
  cluster_centroids_per_class: null
  fixed_instances_per_class: null
  use_random_sampling: false
  
  # Evaluation settings
  eval_on_test_data: true

# Logging verbosity control
# Set to "verbose" for detailed debug logs, "normal" for standard logs, "quiet" for minimal logs
# Controls logging in training, inference, and environment during rollouts
logging_verbosity: "quiet"  # Options: "quiet", "normal", "verbose"

# Maximum number of steps per episode
max_cycles: 500

# Termination reason counters: disable overused reasons
# Strategy: Higher limits for better outcomes, lower limits for easier/less ideal outcomes
max_termination_count_both_targets: -1
max_termination_count_high_precision: 200
max_termination_count_both_close: 50
max_termination_count_excellent_precision: 30
